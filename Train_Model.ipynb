{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMYBPBfFQppu",
        "outputId": "70defe24-e39f-4843-87eb-f23c946d5293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "from IPython.display import clear_output\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.core.dtypes.cast import maybe_box_datetimelike\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.modules.container import Sequential\n",
        "from torch.utils.data import random_split, TensorDataset, Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "\n",
        "\n",
        "from Model_and_Dataset import CustomStructureDataset, First_CNN\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "22ycyxrCtw_d"
      },
      "outputs": [],
      "source": [
        "! pip install torchmetrics\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MYOt9ZrLdQA2"
      },
      "outputs": [],
      "source": [
        "!pip install lion-pytorch\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MmZT942R9Ks"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0RI78uZK1DB1"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomStructureDataset('drive/MyDrive/Dataset/Data_files_10_clear_added.csv', str_dir = 'drive/MyDrive/Dataset')\n",
        "val_dataset = CustomStructureDataset('drive/MyDrive/Dataset/Data_files_short_clear.csv', str_dir = 'drive/MyDrive/Dataset')\n",
        "test_dataset = CustomStructureDataset('drive/MyDrive/Dataset/Data_files_test_2.csv', str_dir = 'drive/MyDrive/Dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15u4OIhtfq2e",
        "outputId": "32f1c798-d736-42aa-9ff7-7ba013535496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.4862284660339355 2.1827330589294434\n"
          ]
        }
      ],
      "source": [
        "train_labels = torch.Tensor(list(train_dataset.str_labels[2].astype(float)))\n",
        "mean = torch.mean(train_labels).item()\n",
        "std = torch.std(train_labels).item()\n",
        "print(mean, std)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = []\n",
        "for i in train_labels:\n",
        "  delta = abs(i-mean).item()\n",
        "  if delta<=1:    \n",
        "    delta=1\n",
        "  weights.append(delta)"
      ],
      "metadata": {
        "id": "_AXXfAA83p3U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-bWnvGvmkPHe"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataset.normalize = True\n",
        "train_dataset.mean = mean\n",
        "train_dataset.std = std\n",
        "train_dataset.train = True\n",
        "train_dataset.transform = True\n",
        "train_dataset.transform_str = None\n",
        "\n",
        "val_dataset.train = False\n",
        "val_dataset.normalize = True\n",
        "val_dataset.mean = mean\n",
        "val_dataset.std = std\n",
        "val_dataset.transform = None\n",
        "\n",
        "test_dataset.train = False\n",
        "test_dataset.normalize = True\n",
        "test_dataset.mean = mean\n",
        "test_dataset.std = std\n",
        "test_dataset.transform = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFF1zu9cTtMy"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import WeightedRandomSampler\n",
        "batch_size = 32\n",
        "#weightedsampler = WeightedRandomSampler(weights, len(weights)) \n",
        "one_list = [1 for i in range(len(weights))]\n",
        "weightedsampler = WeightedRandomSampler(one_list, len(weights)) \n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler = weightedsampler, num_workers=2)\n",
        "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=1, num_workers=2) \n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=1, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsBpb1HQV4oj"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import PearsonCorrCoef, R2Score, MeanSquaredError\n",
        "\n",
        "def train_func(model, criterion, optimizer, num_epochs, best_metrics):\n",
        "  loss_hist = [] # for plotting\n",
        "  acc_val_list = []\n",
        "  val_loss_hist = [] # for plotting\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      hist_loss = 0\n",
        "      for _, batch in tqdm(enumerate(train_loader, 0)): # get batch\n",
        "          # parse batch \n",
        "          structure, labels = batch\n",
        "          structure, labels = structure.to(device=device, dtype=torch.float), labels.to(device=device, dtype=torch.float)\n",
        "          # sets the gradients of all optimized tensors to zero.\n",
        "          optimizer.zero_grad() \n",
        "          # get outputs\n",
        "          y_pred = model(structure)\n",
        "          y_pred = torch.reshape(y_pred, (-1,))\n",
        "          # calculate loss\n",
        "          loss = criterion(y_pred, labels)\n",
        "          # calculate gradients\n",
        "          loss.backward() \n",
        "          # performs a single optimization step (parameter update)\n",
        "          optimizer.step()\n",
        "          hist_loss += loss.item()\n",
        "      loss_hist.append(hist_loss / len(train_loader))\n",
        "      \n",
        "      pred_list_val = []\n",
        "      labels_list_val = []\n",
        "      pred_list_norm = []\n",
        "      labels_list_norm = []\n",
        "      pearson = PearsonCorrCoef().to(device)\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        for struct_val, labels_val in val_loader:\n",
        "          struct_val, labels_val = struct_val.to(device=device, dtype=torch.float), labels_val.to(device=device, dtype=torch.float)\n",
        "          pred_val = model(struct_val.to(device))       \n",
        "          labels_list_val.append(labels_val[0].item()*std+mean)\n",
        "          pred_list_val.append(torch.reshape(pred_val, (-1,))[0].item()*std+mean)\n",
        "          pred_list_norm.append(torch.reshape(pred_val, (-1,))[0].item())\n",
        "          labels_list_norm.append(labels_val[0].item())\n",
        "      val_loss_hist = criterion(torch.Tensor(pred_list_norm).to(device), torch.Tensor(labels_list_norm).to(device))\n",
        "      corr_coef= pearson(torch.Tensor(pred_list_val).to(device), torch.Tensor(labels_list_val).to(device))\n",
        "      if corr_coef > best_metrics:\n",
        "        best_metrics = corr_coef\n",
        "        best_model = deepcopy(model)\n",
        "      print(f\"Epoch={epoch} loss={loss_hist[epoch]:.4f} val_loss: {round(val_loss_hist.item(),3)} val_corr: {round(corr_coef.item(),3)}\")\n",
        "  return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlYJV-6iXltv",
        "outputId": "1b276171-e18b-4837-8c8c-78e4921a39e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:23,  1.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 loss=0.9650 val_loss: 0.778 val_corr: -0.004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=1 loss=0.8585 val_loss: 0.8 val_corr: 0.094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:25,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=2 loss=0.7428 val_loss: 0.834 val_corr: -0.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:23,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=3 loss=0.6594 val_loss: 0.722 val_corr: 0.115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=4 loss=0.6332 val_loss: 0.686 val_corr: 0.191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=5 loss=0.5642 val_loss: 0.689 val_corr: 0.214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:22,  1.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=6 loss=0.5794 val_loss: 0.76 val_corr: 0.076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:23,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=7 loss=0.5629 val_loss: 0.642 val_corr: 0.307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:23,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=8 loss=0.5657 val_loss: 0.693 val_corr: 0.245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:22,  1.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=9 loss=0.5275 val_loss: 0.662 val_corr: 0.31\n"
          ]
        }
      ],
      "source": [
        "from lion_pytorch import Lion\n",
        "set_random_seed(42)\n",
        "model = First_CNN()\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "#criterion = nn.L1Loss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-3)\n",
        "#optimizer = Lion(model.parameters(), lr=1e-5, weight_decay=1e-3)\n",
        "\n",
        "num_epochs = 10\n",
        "best_model = train_func(model, criterion, optimizer, num_epochs, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrMMYjy-FNWA",
        "outputId": "c813bd26-919e-4020-9501-da4ad70ca80e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 loss=0.5296 val_loss: 0.666 val_corr: 0.332\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=1 loss=0.5169 val_loss: 0.667 val_corr: 0.328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:25,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=2 loss=0.5368 val_loss: 0.587 val_corr: 0.427\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:26,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=3 loss=0.5008 val_loss: 0.633 val_corr: 0.398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=4 loss=0.4750 val_loss: 0.605 val_corr: 0.408\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=5 loss=0.4371 val_loss: 0.556 val_corr: 0.469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:23,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=6 loss=0.4614 val_loss: 0.667 val_corr: 0.449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=7 loss=0.4283 val_loss: 0.569 val_corr: 0.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:26,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=8 loss=0.4324 val_loss: 0.555 val_corr: 0.472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=9 loss=0.4346 val_loss: 0.52 val_corr: 0.524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=10 loss=0.4094 val_loss: 0.54 val_corr: 0.513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:23,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=11 loss=0.4011 val_loss: 0.552 val_corr: 0.481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=12 loss=0.3833 val_loss: 0.542 val_corr: 0.491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:25,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=13 loss=0.3849 val_loss: 0.579 val_corr: 0.468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:25,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=14 loss=0.3892 val_loss: 0.556 val_corr: 0.463\n"
          ]
        }
      ],
      "source": [
        "best_model.train()\n",
        "optimizer = torch.optim.AdamW(best_model.parameters(), lr=0.0001, weight_decay=1e-3)\n",
        "best_model2 = train_func(best_model, criterion, optimizer, 15, 0.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmGM1raJp8WM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7bdaf34-3a9f-4542-85fe-36fc421b4d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:22,  1.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 loss=0.3398 val_loss: 0.505 val_corr: 0.54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:22,  1.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=1 loss=0.3513 val_loss: 0.507 val_corr: 0.534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:25,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=2 loss=0.3544 val_loss: 0.5 val_corr: 0.542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:22,  1.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=3 loss=0.3135 val_loss: 0.507 val_corr: 0.534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=4 loss=0.3244 val_loss: 0.512 val_corr: 0.534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:26,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=5 loss=0.3362 val_loss: 0.509 val_corr: 0.532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=6 loss=0.3300 val_loss: 0.508 val_corr: 0.531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=7 loss=0.3213 val_loss: 0.516 val_corr: 0.521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:26,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=8 loss=0.3066 val_loss: 0.513 val_corr: 0.529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:25,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=9 loss=0.3208 val_loss: 0.512 val_corr: 0.525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:27,  1.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=10 loss=0.2984 val_loss: 0.531 val_corr: 0.513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:25,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=11 loss=0.2762 val_loss: 0.529 val_corr: 0.511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=12 loss=0.3116 val_loss: 0.531 val_corr: 0.502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=13 loss=0.2889 val_loss: 0.534 val_corr: 0.504\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "103it [02:24,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=14 loss=0.3334 val_loss: 0.537 val_corr: 0.494\n"
          ]
        }
      ],
      "source": [
        "best_model2.train()\n",
        "optimizer = torch.optim.AdamW(best_model2.parameters(), lr=0.00001, weight_decay=1e-3)\n",
        "best_model2 = train_func(best_model2, criterion, optimizer, 15, 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVVHCnojZpi0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def validate(model, test_loader, device = \"cpu\"):\n",
        "  model.eval()\n",
        "  pred_list = []\n",
        "  labels_list = []\n",
        "  for struct, labels in test_loader:\n",
        "      struct, labels = struct.to(device=device, dtype=torch.float), labels.to(device=device, dtype=torch.float)\n",
        "      pred = model(struct.to(device))     \n",
        "      labels_list.append(labels[0].item()*std+mean)\n",
        "      pred_list.append(round(torch.reshape(pred, (-1,))[0].item()*std+mean, 2))\n",
        "\n",
        "  pearson = PearsonCorrCoef().to(device)\n",
        "  corr_coef= pearson(torch.Tensor(pred_list).to(device), torch.Tensor(labels_list).to(device))\n",
        "\n",
        "  mse = MeanSquaredError().to(device)\n",
        "  rmse = math.sqrt(mse(torch.Tensor(pred_list).to(device), torch.Tensor(labels_list).to(device)).item())\n",
        "  \n",
        "  print(f'PearsonCorr: {round(corr_coef.item(),2)}')\n",
        "  print(f'RMSE: {round(rmse,3)}')\n",
        "  return labels_list, pred_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cfrD7nms2rI",
        "outputId": "eb7f117b-c57d-4018-ff76-4b619d6112d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PearsonCorr: 0.55\n",
            "RMSE: 1.534\n"
          ]
        }
      ],
      "source": [
        "a, b = validate(best_model2, test_loader, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMoGh31Q5l38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23db0c9c-ca32-400e-b8cb-4896ce31ebac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Pred_pKD  True_pKD\n",
              "0        7.23      9.16\n",
              "1        6.87      8.41\n",
              "2        7.26      5.00\n",
              "3        6.47      6.60\n",
              "4        7.05      7.48\n",
              "5        8.58      9.62\n",
              "6        7.77     10.33\n",
              "7        6.33      5.46\n",
              "8        6.69      6.37\n",
              "9        8.31      7.98\n",
              "10       6.75      6.74\n",
              "11       6.62      6.82\n",
              "12       8.94      9.77\n",
              "13       6.74      6.73\n",
              "14       7.39      9.03\n",
              "15       8.42      9.20\n",
              "16       7.02      4.52\n",
              "17       6.10      4.40\n",
              "18       7.89      9.47\n",
              "19       8.70      8.94\n",
              "20       8.91     10.95\n",
              "21       8.79      7.07\n",
              "22       9.35      9.42\n",
              "23       8.74      9.91\n",
              "24       8.10      9.72\n",
              "25       7.37      4.80\n",
              "26       9.07     10.55\n",
              "27       7.83      8.00\n",
              "28       7.58      8.59\n",
              "29       7.42      8.55\n",
              "30       5.60      4.12\n",
              "31       9.01     10.79\n",
              "32       6.73      6.02\n",
              "33       9.08     11.10\n",
              "34       7.24      9.68\n",
              "35       6.42      4.24\n",
              "36       6.00      2.73\n",
              "37       7.44      7.64\n",
              "38       8.84      7.97\n",
              "39       7.19      6.57\n",
              "40       7.41      7.18\n",
              "41       8.27      8.95\n",
              "42       7.71      5.24\n",
              "43       7.43      7.80\n",
              "44       8.05      8.54\n",
              "45       7.06      7.60\n",
              "46       7.23      7.57\n",
              "47       6.96      8.19\n",
              "48       7.00      7.74\n",
              "49       7.58      6.74\n",
              "50       8.36      8.82\n",
              "51       5.74     10.80\n",
              "52       8.34      8.73\n",
              "53       6.99      6.85\n",
              "54       6.66      6.17\n",
              "55       6.60      5.52\n",
              "56       6.44      4.70\n",
              "57       6.75      5.82\n",
              "58       6.45      5.34\n",
              "59       8.51      5.43\n",
              "60       8.02      8.05\n",
              "61       8.66      6.33\n",
              "62       7.18      7.68\n",
              "63       7.50      8.03\n",
              "64       6.70      7.21\n",
              "65       6.67      6.52\n",
              "66       7.41      9.38\n",
              "67       8.34      9.82\n",
              "68       8.96      7.72\n",
              "69       8.97      7.77\n",
              "70       6.71      8.60\n",
              "71       7.35      7.83\n",
              "72       7.11      4.68\n",
              "73       7.31      8.60\n",
              "74       7.12      9.40\n",
              "75       6.62     12.14\n",
              "76       8.65      9.73\n",
              "77       9.27      9.85\n",
              "78       7.77      7.40\n",
              "79       8.89      7.16\n",
              "80       8.65      7.79\n",
              "81       6.48      6.95\n",
              "82       6.93      5.47\n",
              "83       6.88      4.57\n",
              "84       6.88      3.99\n",
              "85       6.62      6.12\n",
              "86       6.76      5.55\n",
              "87       7.72      8.74\n",
              "88       9.28     10.07\n",
              "89       6.95      8.00\n",
              "90       7.97      8.84\n",
              "91       8.49     10.56\n",
              "92       7.07      7.75\n",
              "93       6.69      6.27\n",
              "94       5.69      5.43\n",
              "95       7.35      5.63\n",
              "96       6.95      5.80\n",
              "97       7.25      6.88\n",
              "98       7.25      3.55\n",
              "99       9.15      7.74\n",
              "100      7.98      9.18\n",
              "101      9.15     10.76\n",
              "102      7.73      3.80\n",
              "103      5.04      6.57\n",
              "104      5.98      5.97\n",
              "105      7.62      4.91\n",
              "106      6.54      8.13\n",
              "107      8.31      8.23\n",
              "108      8.65      8.00\n",
              "109      8.01      7.59\n",
              "110      7.75      5.37\n",
              "111      6.56      7.54\n",
              "112      6.80      6.74\n",
              "113      6.65      9.57\n",
              "114      6.20      8.38\n",
              "115      8.32     10.74\n",
              "116      7.79      8.19\n",
              "117      7.91      8.54\n",
              "118      6.82      6.05\n",
              "119      9.31      7.24\n",
              "120      7.98      7.28\n",
              "121      7.67      9.00\n",
              "122      7.93      9.79\n",
              "123      6.53      6.37\n",
              "124      4.95      4.83\n",
              "125      7.21      6.37\n",
              "126      5.19      3.70\n",
              "127      5.19      4.39\n",
              "128      8.85      8.17\n",
              "129      8.59      7.80\n",
              "130      8.40      8.43\n",
              "131      7.43      7.39\n",
              "132      7.25      7.51\n",
              "133      7.83      6.75\n",
              "134      7.32      8.09\n",
              "135      8.54      7.92\n",
              "136      8.17      9.46\n",
              "137      6.87      7.81\n",
              "138      6.35      9.07\n",
              "139      8.26      6.75\n",
              "140      8.07      8.59\n",
              "141      7.92      8.82\n",
              "142      7.40      9.10\n",
              "143      7.91      6.95\n",
              "144      7.88      8.39\n",
              "145      8.63      9.28\n",
              "146      6.34      7.04\n",
              "147      7.51      6.68\n",
              "148      7.80      7.85\n",
              "149      7.63      7.40\n",
              "150      6.65      7.89\n",
              "151      6.64      7.60\n",
              "152      5.90      6.38\n",
              "153      6.53      7.40\n",
              "154      6.48      8.46\n",
              "155      6.19      4.54\n",
              "156      8.37      7.56\n",
              "157      6.76      5.94\n",
              "158      6.64      4.84\n",
              "159      6.29      5.16\n",
              "160      6.61      4.98\n",
              "161      7.56     11.70\n",
              "162      8.43      9.38\n",
              "163      8.43      9.03\n",
              "164      8.78      8.24\n",
              "165      5.63      6.96\n",
              "166      5.87      7.38\n",
              "167      6.73      4.73\n",
              "168      8.30      8.49\n",
              "169      7.70      8.43\n",
              "170      6.67      4.12\n",
              "171      7.64      9.62\n",
              "172      8.32      9.09\n",
              "173      8.14      9.07"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5b252d15-2775-46f9-8421-a425b1b0a7c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pred_pKD</th>\n",
              "      <th>True_pKD</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.23</td>\n",
              "      <td>9.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.87</td>\n",
              "      <td>8.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.26</td>\n",
              "      <td>5.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.47</td>\n",
              "      <td>6.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.05</td>\n",
              "      <td>7.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8.58</td>\n",
              "      <td>9.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7.77</td>\n",
              "      <td>10.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6.33</td>\n",
              "      <td>5.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>6.69</td>\n",
              "      <td>6.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8.31</td>\n",
              "      <td>7.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>6.75</td>\n",
              "      <td>6.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>6.62</td>\n",
              "      <td>6.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>8.94</td>\n",
              "      <td>9.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>6.74</td>\n",
              "      <td>6.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>7.39</td>\n",
              "      <td>9.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>8.42</td>\n",
              "      <td>9.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>7.02</td>\n",
              "      <td>4.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>6.10</td>\n",
              "      <td>4.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>7.89</td>\n",
              "      <td>9.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>8.70</td>\n",
              "      <td>8.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>8.91</td>\n",
              "      <td>10.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>8.79</td>\n",
              "      <td>7.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>9.35</td>\n",
              "      <td>9.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>8.74</td>\n",
              "      <td>9.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>8.10</td>\n",
              "      <td>9.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>7.37</td>\n",
              "      <td>4.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9.07</td>\n",
              "      <td>10.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>7.83</td>\n",
              "      <td>8.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>7.58</td>\n",
              "      <td>8.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>7.42</td>\n",
              "      <td>8.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>5.60</td>\n",
              "      <td>4.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>9.01</td>\n",
              "      <td>10.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>6.73</td>\n",
              "      <td>6.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>9.08</td>\n",
              "      <td>11.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>7.24</td>\n",
              "      <td>9.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>6.42</td>\n",
              "      <td>4.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>6.00</td>\n",
              "      <td>2.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>7.44</td>\n",
              "      <td>7.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>8.84</td>\n",
              "      <td>7.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>7.19</td>\n",
              "      <td>6.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>7.41</td>\n",
              "      <td>7.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>8.27</td>\n",
              "      <td>8.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>7.71</td>\n",
              "      <td>5.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>7.43</td>\n",
              "      <td>7.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>8.05</td>\n",
              "      <td>8.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>7.06</td>\n",
              "      <td>7.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>7.23</td>\n",
              "      <td>7.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>6.96</td>\n",
              "      <td>8.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>7.00</td>\n",
              "      <td>7.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>7.58</td>\n",
              "      <td>6.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>8.36</td>\n",
              "      <td>8.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>5.74</td>\n",
              "      <td>10.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>8.34</td>\n",
              "      <td>8.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>6.99</td>\n",
              "      <td>6.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>6.66</td>\n",
              "      <td>6.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>6.60</td>\n",
              "      <td>5.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>6.44</td>\n",
              "      <td>4.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>6.75</td>\n",
              "      <td>5.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>6.45</td>\n",
              "      <td>5.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>8.51</td>\n",
              "      <td>5.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>8.02</td>\n",
              "      <td>8.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>8.66</td>\n",
              "      <td>6.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>7.18</td>\n",
              "      <td>7.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>7.50</td>\n",
              "      <td>8.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>6.70</td>\n",
              "      <td>7.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>6.67</td>\n",
              "      <td>6.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>7.41</td>\n",
              "      <td>9.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>8.34</td>\n",
              "      <td>9.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>8.96</td>\n",
              "      <td>7.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>8.97</td>\n",
              "      <td>7.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>6.71</td>\n",
              "      <td>8.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>7.35</td>\n",
              "      <td>7.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>7.11</td>\n",
              "      <td>4.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>7.31</td>\n",
              "      <td>8.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>7.12</td>\n",
              "      <td>9.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>6.62</td>\n",
              "      <td>12.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>8.65</td>\n",
              "      <td>9.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>9.27</td>\n",
              "      <td>9.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>7.77</td>\n",
              "      <td>7.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>8.89</td>\n",
              "      <td>7.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>8.65</td>\n",
              "      <td>7.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>6.48</td>\n",
              "      <td>6.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>6.93</td>\n",
              "      <td>5.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>6.88</td>\n",
              "      <td>4.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>6.88</td>\n",
              "      <td>3.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>6.62</td>\n",
              "      <td>6.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>6.76</td>\n",
              "      <td>5.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>7.72</td>\n",
              "      <td>8.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>9.28</td>\n",
              "      <td>10.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>6.95</td>\n",
              "      <td>8.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>7.97</td>\n",
              "      <td>8.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>8.49</td>\n",
              "      <td>10.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>7.07</td>\n",
              "      <td>7.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>6.69</td>\n",
              "      <td>6.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>5.69</td>\n",
              "      <td>5.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>7.35</td>\n",
              "      <td>5.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>6.95</td>\n",
              "      <td>5.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>7.25</td>\n",
              "      <td>6.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>7.25</td>\n",
              "      <td>3.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>9.15</td>\n",
              "      <td>7.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>7.98</td>\n",
              "      <td>9.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>9.15</td>\n",
              "      <td>10.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>7.73</td>\n",
              "      <td>3.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>5.04</td>\n",
              "      <td>6.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>5.98</td>\n",
              "      <td>5.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>7.62</td>\n",
              "      <td>4.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>6.54</td>\n",
              "      <td>8.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>8.31</td>\n",
              "      <td>8.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>8.65</td>\n",
              "      <td>8.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>8.01</td>\n",
              "      <td>7.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>7.75</td>\n",
              "      <td>5.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>6.56</td>\n",
              "      <td>7.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>6.80</td>\n",
              "      <td>6.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>6.65</td>\n",
              "      <td>9.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>6.20</td>\n",
              "      <td>8.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>8.32</td>\n",
              "      <td>10.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>7.79</td>\n",
              "      <td>8.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>7.91</td>\n",
              "      <td>8.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>6.82</td>\n",
              "      <td>6.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>9.31</td>\n",
              "      <td>7.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>7.98</td>\n",
              "      <td>7.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>7.67</td>\n",
              "      <td>9.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>7.93</td>\n",
              "      <td>9.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>6.53</td>\n",
              "      <td>6.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>4.95</td>\n",
              "      <td>4.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>7.21</td>\n",
              "      <td>6.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>5.19</td>\n",
              "      <td>3.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>5.19</td>\n",
              "      <td>4.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>8.85</td>\n",
              "      <td>8.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>8.59</td>\n",
              "      <td>7.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>8.40</td>\n",
              "      <td>8.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>7.43</td>\n",
              "      <td>7.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>7.25</td>\n",
              "      <td>7.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>7.83</td>\n",
              "      <td>6.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>7.32</td>\n",
              "      <td>8.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>8.54</td>\n",
              "      <td>7.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>8.17</td>\n",
              "      <td>9.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>6.87</td>\n",
              "      <td>7.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>6.35</td>\n",
              "      <td>9.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>8.26</td>\n",
              "      <td>6.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>8.07</td>\n",
              "      <td>8.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>7.92</td>\n",
              "      <td>8.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>7.40</td>\n",
              "      <td>9.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>7.91</td>\n",
              "      <td>6.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>7.88</td>\n",
              "      <td>8.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>8.63</td>\n",
              "      <td>9.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.34</td>\n",
              "      <td>7.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>7.51</td>\n",
              "      <td>6.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>7.80</td>\n",
              "      <td>7.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>7.63</td>\n",
              "      <td>7.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>6.65</td>\n",
              "      <td>7.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>6.64</td>\n",
              "      <td>7.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>5.90</td>\n",
              "      <td>6.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>6.53</td>\n",
              "      <td>7.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>6.48</td>\n",
              "      <td>8.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>6.19</td>\n",
              "      <td>4.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>8.37</td>\n",
              "      <td>7.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>6.76</td>\n",
              "      <td>5.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>6.64</td>\n",
              "      <td>4.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>6.29</td>\n",
              "      <td>5.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>6.61</td>\n",
              "      <td>4.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>7.56</td>\n",
              "      <td>11.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>8.43</td>\n",
              "      <td>9.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>8.43</td>\n",
              "      <td>9.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>8.78</td>\n",
              "      <td>8.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>5.63</td>\n",
              "      <td>6.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>5.87</td>\n",
              "      <td>7.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>6.73</td>\n",
              "      <td>4.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>8.30</td>\n",
              "      <td>8.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>7.70</td>\n",
              "      <td>8.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>6.67</td>\n",
              "      <td>4.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>7.64</td>\n",
              "      <td>9.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>8.32</td>\n",
              "      <td>9.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>8.14</td>\n",
              "      <td>9.07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b252d15-2775-46f9-8421-a425b1b0a7c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5b252d15-2775-46f9-8421-a425b1b0a7c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5b252d15-2775-46f9-8421-a425b1b0a7c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "import pandas as pd\n",
        "corr_df = pd.DataFrame(columns=['Pred_pKD', 'True_pKD'])\n",
        "corr_df['Pred_pKD'] = b\n",
        "corr_df['True_pKD'] = a\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "corr_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3DYUrmin7PK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "outputId": "4ed33711-63ee-48e5-eb33-baa1f9a5c3c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.JointGrid at 0x7fe68666e530>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "sns.jointplot(data=corr_df, x='True_pKD', y='Pred_pKD', palette='Set2', ylim=(2, 14), xlim=(2, 14))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofuz9qDPS2fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe8bd019-7e21-4658-88b1-91993eb5265f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1.1656347695620752,\n",
              " 1: 0.2952499957813757,\n",
              " 2: 0.08289057440708802,\n",
              " 3: -1.21824255636011,\n",
              " 4: 0.48452689535736226,\n",
              " 5: 1.2399541571388113,\n",
              " 6: 0.43974171686659247,\n",
              " 7: 1.0073162300003489,\n",
              " 8: 1.9560435836083343,\n",
              " 9: 0.09615690773637198}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "import numpy as np\n",
        "params = []\n",
        "sum_of_weights={el:0 for el in np.arange(10)}\n",
        "i = 0\n",
        "summarised_weight = 0\n",
        "for param in best_model2.conv_stack[0].parameters():\n",
        "    params.append(param.view(-1))\n",
        "params = torch.cat(params)\n",
        "params_list = params.tolist()\n",
        "\n",
        "sum_of_weights={el:0 for el in np.arange(10)}\n",
        "i = 0\n",
        "summarised_weight = 0\n",
        "for param in params_list:\n",
        "    sum_of_weights[i%10] += param\n",
        "    i += 1\n",
        "sum_of_weights\n",
        "# 0:   1  +  2 \n",
        "# 1:   2  +  1 \n",
        "# 2:   1  +   2 \n",
        "# 3:   2  +   1 \n",
        "# 4: .   1  + .   2 \n",
        "# 5: .   2  + .   1 \n",
        "# 6:   1  +   2 \n",
        "# 7:   1  +   2 \n",
        "# 8:   1  +   2 \n",
        "# 9:   1  +   2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpVSiS77dJju"
      },
      "outputs": [],
      "source": [
        "torch.save(best_model2, 'drive/MyDrive/Model/model_best_55_weight_mol_dyn.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
